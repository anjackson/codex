{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "These are some notes I'm making while experimenting with scaling the indexer.\n",
      "\n",
      "Scaling problems\n",
      "================\n",
      "\n",
      "We ran with 10,000 W/ARCs, but got some troubling timings. Having tweaked that task number, 5 allowed with 1GB RAM each (TODO Add config details), we have a reasonably fast map phase, taking about two hours to process all 10,000 (and so implying up to 90 hours to process them all, but keep in mind that the total is only as fast as the slowest jobs, i.e. the big WARC files dominate at smaller job sizes, and there was some competition for cluster time). The first time the JISC 1996-2010 collection was indexed, it only required about a soild day's worth of processing time, i.e. about 26 hours.\n",
      "\n",
      "<pre>\n",
      "| # Inputs | # Reducers | Finish Time (Map)   | Finish Time (Map+Reduce) |\n",
      "--------------------------------------------------------------------------\n",
      "|    100   | 10         | 8mins, 41sec        | 8mins, 56sec             | 4.4.0\n",
      "|  1,000   | 10         | 31mins, 3sec        | 1hrs, 24mins, 19sec      | 4.6.1\n",
      "| 10,000   | 10         | 2hrs, 45mins, 21sec | 21hrs, 25mins, 56sec     | 4.4.0\n",
      "\n",
      "</pre>\n",
      "\n",
      "However, the 10 reducers are failing. They run twice, the first time crashing out with:\n",
      "\n",
      "<pre>\n",
      "Error: java.io.IOException: No space left on device\n",
      "\tat java.io.FileOutputStream.writeBytes(Native Method)\n",
      "\tat java.io.FileOutputStream.write(FileOutputStream.java:282)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.write(RawLocalFileSystem.java:199)\n",
      "\tat java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:65)\n",
      "\tat java.io.BufferedOutputStream.write(BufferedOutputStream.java:104)\n",
      "\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.write(FSDataOutputStream.java:49)\n",
      "\tat java.io.DataOutputStream.write(DataOutputStream.java:90)\n",
      "\tat org.apache.hadoop.mapred.IFileOutputStream.write(IFileOutputStream.java:84)\n",
      "\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.write(FSDataOutputStream.java:49)\n",
      "\tat java.io.DataOutputStream.write(DataOutputStream.java:90)\n",
      "\tat org.apache.hadoop.mapred.IFile$Writer.append(IFile.java:226)\n",
      "\tat org.apache.hadoop.mapred.Merger.writeFile(Merger.java:157)\n",
      "\tat org.apache.hadoop.mapred.ReduceTask$ReduceCopier$InMemFSMergeThread.doInMemMerge(ReduceTask.java:2699)\n",
      "\tat org.apache.hadoop.mapred.ReduceTask$ReduceCopier$InMemFSMergeThread.run(ReduceTask.java:2640)\n",
      "</pre>\n",
      "\n",
      "then running ok the second time, presumably because there is only enough disk space on some of the machines. [Others have hit this problem](https://issues.apache.org/jira/browse/HADOOP-6092), and this indicates that we might be able to sort things out by clearing up the temporary space that Hadoop is configured to use (TODO add config param info). However, it also implies we are likely to hit an upper limit on the size of job we can process due to the limited about of temp space we have. Note that this should be nothing to do with HDFS free space, because the system temp space is usually held on a different drive to the DFS volumes.\n",
      "\n",
      "<pre>\n",
      "| # Inputs | # Reducers | Shuffle Time         | Sort Time     | Total Reduce Time   |\n",
      "--------------------------------------------------------------------------------------\n",
      "|    100   | 10         | 5mins, 21sec         | 30sec         | 8mins, 51sec        |\n",
      "|  1,000   | 10         | 49mins, 9sec         | 23sec         | 1hrs, 18mins, 33sec | 4.6.1\n",
      "| 10,000   | 10         | 10hrs, 41mins, 35sec | 0sec          | 10hrs, 42mins, 9sec |\n",
      "|          |            | 1hrs, 19mins, 15sec  | 43mins, 12sec | 8hrs, 36mins, 27sec |\n",
      "</pre>\n",
      "\n",
      "Similarly, it's worth noting that even when it works, the sort is taking 7-10 hours (which is why it takes over 20 hours when it fails once). Somewhat oddly, every single reducer failed on disk space the first time around, i.e. roughtly simultanously, and then worked (no clear correlation between failed nodes the first time, i.e. the same rack etc.). That implies that some kind of temp-space job contention might be the issue.\n",
      "\n",
      "Note that there is a lot of lines like this:\n",
      "\n",
      "<pre>\n",
      "ERROR WARCIndexerReducer - No appropriate response record found for: sha1:223JBF7A4BH6TNGCAI2MIPGWOPBNJLBB_http://news.bbcimg.co.uk/media/images/48244000/jpg/_48244565_lorenzo_reuters226i.jpg (revisit)\n",
      "</pre>\n",
      "\n",
      "This is a consequence of the fact that that the small sample means the deduplication strategy is failing. Some WARCs are mostly revisits.\n",
      "\n",
      "TODO What is the ARC/WARC composition of the 10,000?\n",
      "ARC 5593\n",
      "WARC 4407\n",
      "\n",
      "ARC 442703\n",
      "WARC 4494\n",
      "\n",
      "https://issues.apache.org/jira/browse/SOLR-4816 means we are suffering on indexing throughput.\n",
      "\n",
      "https://wiki.apache.org/solr/SolrCloud\n",
      "\n",
      "Also, it seems we are putting too much pressure on the sort now. Perhaps partly due to the link extraction and partly due to the higher binary limit allowing more resources to use up more of the 1MB text field size limit.\n",
      "\n",
      "\n",
      "Deduplication strategies\n",
      "========================\n",
      "\n",
      "We are using the reduce step as out deduplication strategy. Items with the same URL and content hash are grouped together, and only a single SOLR record is submitted for each one.\n",
      "\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}