{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "These are some notes I'm making while experimenting with scaling the indexer.\n",
      "\n",
      "Scaling problems\n",
      "================\n",
      "\n",
      "We ran with 10,000 W/ARCs, but got some troubling timings. Having tweaked that task number, 5 allowed with 1GB RAM each (TODO Add config details), we have a reasonably fast map phase, taking about two hours to process all 10,000 (and so implying up to 90 hours to process them all, but keep in mind that the total is only as fast as the slowest jobs, i.e. the big WARC files dominate at smaller job sizes, and there was some competition for cluster time). The first time the JISC 1996-2010 collection was indexed, it only required about a soild day's worth of processing time, i.e. about 26 hours.\n",
      "\n",
      "<pre>\n",
      "| # Inputs | # Reducers | Finish Time (Map)   | Finish Time (Map+Reduce) |\n",
      "--------------------------------------------------------------------------\n",
      "|    100   | 10         | 8mins, 41sec        | 8mins, 56sec             | 4.4.0\n",
      "|  1,000   | 10         | 31mins, 3sec        | 1hrs, 24mins, 19sec      | 4.6.1\n",
      "| 10,000   | 10         | 2hrs, 45mins, 21sec | 21hrs, 25mins, 56sec     | 4.4.0\n",
      "\n",
      "</pre>\n",
      "\n",
      "However, the 10 reducers are failing. They run twice, the first time crashing out with:\n",
      "\n",
      "<pre>\n",
      "Error: java.io.IOException: No space left on device\n",
      "\tat java.io.FileOutputStream.writeBytes(Native Method)\n",
      "\tat java.io.FileOutputStream.write(FileOutputStream.java:282)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.write(RawLocalFileSystem.java:199)\n",
      "\tat java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:65)\n",
      "\tat java.io.BufferedOutputStream.write(BufferedOutputStream.java:104)\n",
      "\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.write(FSDataOutputStream.java:49)\n",
      "\tat java.io.DataOutputStream.write(DataOutputStream.java:90)\n",
      "\tat org.apache.hadoop.mapred.IFileOutputStream.write(IFileOutputStream.java:84)\n",
      "\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.write(FSDataOutputStream.java:49)\n",
      "\tat java.io.DataOutputStream.write(DataOutputStream.java:90)\n",
      "\tat org.apache.hadoop.mapred.IFile$Writer.append(IFile.java:226)\n",
      "\tat org.apache.hadoop.mapred.Merger.writeFile(Merger.java:157)\n",
      "\tat org.apache.hadoop.mapred.ReduceTask$ReduceCopier$InMemFSMergeThread.doInMemMerge(ReduceTask.java:2699)\n",
      "\tat org.apache.hadoop.mapred.ReduceTask$ReduceCopier$InMemFSMergeThread.run(ReduceTask.java:2640)\n",
      "</pre>\n",
      "\n",
      "then running ok the second time, presumably because there is only enough disk space on some of the machines. [Others have hit this problem](https://issues.apache.org/jira/browse/HADOOP-6092), and this indicates that we might be able to sort things out by clearing up the temporary space that Hadoop is configured to use (TODO add config param info). However, it also implies we are likely to hit an upper limit on the size of job we can process due to the limited about of temp space we have. Note that this should be nothing to do with HDFS free space, because the system temp space is usually held on a different drive to the DFS volumes.\n",
      "\n",
      "<pre>\n",
      "| # Inputs | # Reducers | Shuffle Time         | Sort Time     | Total Reduce Time   |\n",
      "--------------------------------------------------------------------------------------\n",
      "|    100   | 10         | 5mins, 21sec         | 30sec         | 8mins, 51sec        |\n",
      "|  1,000   | 10         | 49mins, 9sec         | 23sec         | 1hrs, 18mins, 33sec | 4.6.1\n",
      "| 10,000   | 10         | 10hrs, 41mins, 35sec | 0sec          | 10hrs, 42mins, 9sec |\n",
      "|          |            | 1hrs, 19mins, 15sec  | 43mins, 12sec | 8hrs, 36mins, 27sec |\n",
      "</pre>\n",
      "\n",
      "Similarly, it's worth noting that even when it works, the sort is taking 7-10 hours (which is why it takes over 20 hours when it fails once). Somewhat oddly, every single reducer failed on disk space the first time around, i.e. roughtly simultanously, and then worked (no clear correlation between failed nodes the first time, i.e. the same rack etc.). That implies that some kind of temp-space job contention might be the issue.\n",
      "\n",
      "Note that there is a lot of lines like this:\n",
      "\n",
      "<pre>\n",
      "ERROR WARCIndexerReducer - No appropriate response record found for: sha1:223JBF7A4BH6TNGCAI2MIPGWOPBNJLBB_http://news.bbcimg.co.uk/media/images/48244000/jpg/_48244565_lorenzo_reuters226i.jpg (revisit)\n",
      "</pre>\n",
      "\n",
      "This is a consequence of the fact that that the small sample means the deduplication strategy is failing. Some WARCs are mostly revisits.\n",
      "\n",
      "TODO What is the ARC/WARC composition of the 10,000?\n",
      "ARC 5593\n",
      "WARC 4407\n",
      "\n",
      "ARC 442703\n",
      "WARC 4494\n",
      "\n",
      "https://issues.apache.org/jira/browse/SOLR-4816 means we are suffering on indexing throughput.\n",
      "\n",
      "https://wiki.apache.org/solr/SolrCloud\n",
      "\n",
      "Also, it seems we are putting too much pressure on the sort now. Perhaps partly due to the link extraction and partly due to the higher binary limit allowing more resources to use up more of the 1MB text field size limit.\n",
      "\n",
      "\n",
      "Deduplication strategies\n",
      "========================\n",
      "\n",
      "We are using the reduce step as out deduplication strategy. Items with the same URL and content hash are grouped together, and only a single SOLR record is submitted for each one.\n",
      "\n",
      "To resolve this, we had to properly calculate the hash of the ARCs and allow for multiple crawl dates, and query Solr during the map to decide whether to send an update to the crawl_dates or not.\n",
      "\n",
      "Rebuilding the indexer\n",
      "======================\n",
      "\n",
      "So, the indexer has been rebuilt. \n",
      "\n",
      "* Uses new duplicate handling logic.\n",
      "* Requests compression of the map output.\n",
      "* Face detection, colour extraction.\n",
      "* ...\n",
      "\n",
      "All 'expensive' features are switched on.\n",
      "\n",
      "Now we need new timings. Started with ten inputs, but a new ten, so numbers will not be directly comparable.\n",
      "\n",
      "For 10, Total time: 00:21:52.\n",
      "Map, Worst case 00:20:00, most most around 00:08:00.\n",
      "Reduce time, 18 mins but this includes the long running shuffle and sort while awaiting the slowest map.\n",
      "Actual reduce action time was 30-40 seconds.\n",
      "\n",
      "On 100, hit problems with empty/malformed payloads that killed the job. Fixing this and re-launching.\n",
      "\n",
      "We are getting DEBUG output from org.apache.zookeeper and it's not clear why.\n",
      "\n",
      "Rather slow to warm up and get going. Hopefully this is mapper initialisation and we'll pick up speed shortly. OK Looks like SOLR crashed, and the clients are waiting for it. It was an OOM, but actually 'unable to create new native thread', which is a ulimit thing. Need up up the ulimits for the tomcat user and restart the cluster.\n",
      "\n",
      "Ok, re-running with SOLR rebooted.\n",
      "\n",
      "For 100:\n",
      "Total time: 00:43:21\n",
      "Maps, 6-20 mins per input.\n",
      "Reducers, 38 mins overall, but actual submission to Solr only about 2 minutes. (Seems much faster.)\n",
      "This would mean 20 weeks! Need timings from 1000 to confirm relilability of this estimate.\n",
      "\n",
      "For 1000:\n",
      "Some contention, other indexing jobs running at the same time.\n",
      "Now ArchiveCDXGenerator and sorter jobs kicked in, all competing for map time.\n",
      "Total time: 15:43:40\n",
      "Maps: Most 1-2 hrs, worst case was around 11 hours!\n",
      "Reduce phase took about two hours, but was overlapping with another job in the reduce phase.\n",
      "\n",
      "Files taking many hours (>8 hours) to map\n",
      "Processing path: hdfs://nellie-private:54310/ia/PHASE2WARCS/DOTUK-HISTORICAL-1996-2010-PHASE2WARCS-XAAAAZ-20111115000000-000000.warc.gz\n",
      "\n",
      "\n",
      "\n",
      "Speeding things up\n",
      "------------------\n",
      "\n",
      "To speed things up, we can go to the other extreme and try switching off lots of features.\n",
      "\n",
      "\n",
      "* Disabling both Image and PDF analysis.\n",
      "    * Local test: 77.89 seconds -> 45.185 seconds.\n",
      "* Disabling PDF analysis.\n",
      "    * Local test: 77.89 seconds -> 74.764 seconds.\n",
      "* Disabling Image analysis.\n",
      "    * Local test: 77.89 seconds -> 51.093 seconds.\n",
      "* Up the limit on in-memory content processing from 1MB to 10MB:\n",
      "    * Local test: 81.992 seconds -> 77.89 seconds.\n",
      "* Dropping maximum text to extract from 1024K to 1K:\n",
      "    * Local test: 45.185 seconds -> 42.631 seconds.\n",
      "* Dropping maximum bytes to allow Tika to parse from ALL to 1K:\n",
      "    * Local test: 64.982 seconds -> 53.949 seconds.\n",
      "    \n",
      "So, dropping the image analysis made a large difference. Given the pressures involved right now, it probably makes more sense to disable these features (which are of relatively little interest to the main BUDDAH researchers right now). \n",
      "\n",
      "Rerunning with image and PDF features turned off.\n",
      "\n",
      "For 1000:\n",
      "Note some contention with previous job, but mostly in the reducer phase.\n",
      "Total time: 10:27:01.\n",
      "Mappers better behaved, worst case now 04:20:13.\n",
      "Reduce phase approx 3.5 hours, but heavy contention with other reduces from previous job.\n",
      "\n",
      "4hr worst case map:\n",
      "hdfs://nellie-private:54310/ia/PHASE2WARCS/DOTUK-HISTORICAL-1996-2010-PHASE2WARCS-XAABLX-20111115000000-000000.warc.gz\n",
      "\n",
      "Also bad: \n",
      "hdfs://nellie-private:54310/ia/PHASE2WARCS/DOTUK-HISTORICAL-1996-2010-PHASE2WARCS-XAAANR-20111115000000-000001.warc.gz\n",
      "hdfs://nellie-private:54310/ia/PHASE2WARCS/DOTUK-HISTORICAL-1996-2010-PHASE2WARCS-XAAAAZ-20111115000000-000000.warc.gz\n",
      "\n",
      "So, setting up a local test with the worst file (XAAAAZ-20111115000000-000000, which is only 0.6GB, so it's probably not raw size that's the problem).\n",
      "Hang on, that's a bad idea, cos it's probably take 4 hours!\n",
      "Adding logging to see where it gets stuck...\n",
      "Oh dear. It worked fine.\n",
      "\"Finished in 2791.518 seconds.\"\n",
      "\n",
      "Running again, but excluding most formats from Tika processing (as we usually do): 2749.475 seconds!\n",
      "Didn't beleve it, so disabled the excludes (i.e. all in) once more: 2018.007 seconds!?\n",
      "\n",
      "Ok, so cleaning up code and disabling Tika for problematic types, and rerunning on the 100 with no contention gives...\n",
      "Total time: 00:32:07\n",
      "Mappers roughly 20-30mins.\n",
      "Reducers e.g. 00:01:22 i.e. around a minute.\n",
      "\n",
      "Trying again with the 1000, although with some contention and HDFS is extremely full which may be causing problems...\n",
      "Total time:\n",
      "\n",
      "\n",
      "Other things to try:\n",
      "- Disable recursive parsing in Tika.\n",
      "- ONLY hash the first X bytes, and use that for dedup.\n",
      "\n",
      "100, 30 mins?\n",
      "\n",
      "Okay, so back on the cluster, with the solr check switched off (relying on updates instead of managing that myself and querying for every resources), and WHOA that's better. The shuf-100 job runs in about 10mins (instead of 30mins)\n",
      "Total time:\n",
      "Mappers:  10 mins.\n",
      "Reducers:  FAILED\n",
      "\n",
      "Still pretty slow on the reduce. 35 might be hammering it, but still. Maybe need to try cutting down on some fields, e.g. href indexing, possibly even worth ignoring host-level links and just do domain-level.\n",
      "\n",
      "Ah, no, my fault. It's the new crawl_years field. Nice work Jackson.\n",
      "So, taken that out.\n",
      "\n",
      "Ran again with 100 inputs. Mapper nice and quick, and 35 reducers coped this time.\n",
      "Total time: 00:17:52\n",
      "Mappers: c. 8 mins.\n",
      "Reducers: c. 10 mins.\n",
      "\n",
      "Ran with 1000 inputs, mappers quite quick, but reducers kept dying. Dropping the number of reducers to 20, and it seems stable.\n",
      "\n",
      "Total time: > 01:57:00\n",
      "Mappers: c. 30 mins.\n",
      "Reducers: lots more time: > 1hr. Hmmm, after about 94.33% of the reduce phase, Solr is locking up. Kill the job and it slowly recovers, which is good.\n",
      "\n",
      "So, things to try next: switch off link analysis. \n",
      "Trying it locally, on the XAAAAZ-20111115000000-000000 test file.\n",
      "Finished in 3800.776 seconds.\n",
      "Hmmm. Totes inconclusive due to variability of timings on the laptop.\n",
      "\n",
      "So, on the cluster, and without links, for the 1000:\n",
      "After ten minutes, 85% done!\n",
      "After 15 minutes, 99% done!\n",
      "After 20 mins, 99.95% done!\n",
      "After 25 mins, 100% done! (only a couple of WARCs at the end, so more to be gained by running more inputs at once)\n",
      "After c.28 mins, sort is also complete.\n",
      "Reducers stuck after nearlu eight hours! c.94% complete.\n",
      "KILLING\n",
      "\n",
      "So, trying dropping reducers to 10. Still dying, but maybe Solr is very grumpy. Restarting Solr.\n",
      "So, after 01:51:01 it is done.\n",
      "Mappers are quick, index still slow. i.e. about 30-odd mins mappers, about a hour and a quarter indexing.\n",
      "\n",
      "Oh, links were still switched on!? Trying again without the links... Emptying SOLR...\n",
      "\n",
      "Locally, switching off multiple features to see how it changes things. no host links, no binary shingling, no 'elements used':\n",
      "Finished in 2399.014 seconds.\n",
      "Hm. Ok, also dropping text payload right down to 10K:\n",
      "Finished in 2657.414 seconds.\n",
      "\n",
      "So, back on the cluster, with the links off and an empty target Solr:\n",
      "Total time: 01:06:13 i.e. nearly half the time.\n",
      "\n",
      "Also disabled first_bytes, but left data in Solr:\n",
      "Total time: 01:26:03\n",
      "Slightly longer, probably because it was doing updates not just replacements.\n",
      "\n",
      "So, cleared the data out, and also dropped the text payload size down to 10KB:\n",
      "Total time: 01:03:54\n",
      "Nice.\n",
      "\n",
      "Finally, knocking down the number of reducers to 5, to see if that makes much difference. On an empty Solr.\n",
      "Yes, it did run a bit slower, but not massively.\n",
      "Total time: 01:27:06\n",
      "\n",
      "Now running on 10,000, leaving reducers at 5...\n",
      "Map time, about two hours!\n",
      "Reduce Copy kinda slow: reduce > copy (2215 of 10000 at 7.75 MB/s)\n",
      "CRASH with only 5 reducers we run out of disk space...\n",
      "\n",
      "Upping back to 10 reducers.\n",
      "\n",
      "Try 15? First, lets try to see why there's so much data.\n",
      "\n",
      "Dropped the elements_used, in case there was some weirdness there. Was the same (crashed out of disk space during shuffle).\n",
      "\n",
      "Dropping the hosts, in case it's those cheeky link farms that are to blame.\n",
      "\n",
      "Little difference. Trying dropping text load.\n",
      "\n",
      "NOTE Looked in \n",
      "\n",
      "    /mapred/local/dir/taskTracker/anjackson/jobcache/job_201402191107_1551/attempt_201402191107_1551_r_000004_0/output\n",
      " \n",
      "and the output is clearly NOT compressed. And some are BIG, and look like link-farm mess.\n",
      "\n",
      "Even after reducing the text load to 50K, this uncompressed data still failed on some reducers. Eventually some got through, only to cripple the Solr server (still just 10 reducers). Those that got to Solr failed like this.\n",
      "\n",
      "<pre>\n",
      "2014-03-30 16:29:26 INFO  WARCIndexerReducer:111 - Submitted 500 docs [0]\n",
      "2014-03-30 16:30:30 ERROR WARCIndexerReducer:116 - WARCIndexerReducer.reduce(): No live SolrServers available to handle this request:[http://192.168.1.180:8994/solr/jisc3]\n",
      "org.apache.solr.client.solrj.impl.CloudSolrServer$RouteException: No live SolrServers available to handle this request:[http://192.168.1.180:8994/solr/jisc3]\n",
      "\tat org.apache.solr.client.solrj.impl.CloudSolrServer.directUpdate(CloudSolrServer.java:351)\n",
      "\tat org.apache.solr.client.solrj.impl.CloudSolrServer.request(CloudSolrServer.java:510)\n",
      "\tat org.apache.solr.client.solrj.request.AbstractUpdateRequest.process(AbstractUpdateRequest.java:117)\n",
      "\tat org.apache.solr.client.solrj.SolrServer.add(SolrServer.java:68)\n",
      "\tat org.apache.solr.client.solrj.SolrServer.add(SolrServer.java:54)\n",
      "\tat uk.bl.wa.solr.SolrWebServer.add(SolrWebServer.java:106)\n",
      "\tat uk.bl.wa.hadoop.indexer.WARCIndexerReducer.checkSubmission(WARCIndexerReducer.java:110)\n",
      "\tat uk.bl.wa.hadoop.indexer.WARCIndexerReducer.reduce(WARCIndexerReducer.java:84)\n",
      "\tat uk.bl.wa.hadoop.indexer.WARCIndexerReducer.reduce(WARCIndexerReducer.java:29)\n",
      "\tat org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:469)\n",
      "\tat org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:417)\n",
      "\tat org.apache.hadoop.mapred.Child$4.run(Child.java:270)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:396)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1127)\n",
      "\tat org.apache.hadoop.mapred.Child.main(Child.java:264)\n",
      "Caused by: org.apache.solr.client.solrj.SolrServerException: No live SolrServers available to handle this request:[http://192.168.1.180:8994/solr/jisc3]\n",
      "\tat org.apache.solr.client.solrj.impl.LBHttpSolrServer.request(LBHttpSolrServer.java:354)\n",
      "\tat org.apache.solr.client.solrj.impl.CloudSolrServer$1.call(CloudSolrServer.java:332)\n",
      "\tat org.apache.solr.client.solrj.impl.CloudSolrServer$1.call(CloudSolrServer.java:329)\n",
      "\tat java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:138)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)\n",
      "\tat java.lang.Thread.run(Thread.java:662)\n",
      "Caused by: org.apache.solr.client.solrj.impl.HttpSolrServer$RemoteSolrException: Cannot talk to ZooKeeper - Updates are disabled.\n",
      "\tat org.apache.solr.client.solrj.impl.HttpSolrServer.request(HttpSolrServer.java:495)\n",
      "\tat org.apache.solr.client.solrj.impl.HttpSolrServer.request(HttpSolrServer.java:199)\n",
      "\tat org.apache.solr.client.solrj.impl.LBHttpSolrServer.request(LBHttpSolrServer.java:285)\n",
      "\t... 7 more\n",
      "2014-03-30 16:39:49 ERROR WARCIndexerReducer:116 - WARCIndexerReducer.reduce(): No live SolrServers available to handle this request:[http://192.168.1.180:8988/solr/jisc3, http://192.168.1.180:8983/solr/jisc3, http://192.168.1.180:8996/solr/jisc3, http://192.168.1.180:8994/solr/jisc3]\n",
      "org.apache.solr.client.solrj.impl.CloudSolrServer$RouteException: No live SolrServers available to handle this request:[http://192.168.1.180:8988/solr/jisc3, http://192.168.1.180:8983/solr/jisc3, http://192.168.1.180:8996/solr/jisc3, http://192.168.1.180:8994/solr/jisc3]\n",
      "\tat org.apache.solr.client.solrj.impl.CloudSolrServer.directUpdate(CloudSolrServer.java:351)\n",
      "\tat org.apache.solr.client.solrj.impl.CloudSolrServer.request(CloudSolrServer.java:510)\n",
      "\tat org.apache.solr.client.solrj.request.AbstractUpdateRequest.process(AbstractUpdateRequest.java:117)\n",
      "\tat org.apache.solr.client.solrj.SolrServer.add(SolrServer.java:68)\n",
      "\tat org.apache.solr.client.solrj.SolrServer.add(SolrServer.java:54)\n",
      "\tat uk.bl.wa.solr.SolrWebServer.add(SolrWebServer.java:106)\n",
      "\tat uk.bl.wa.hadoop.indexer.WARCIndexerReducer.checkSubmission(WARCIndexerReducer.java:110)\n",
      "\tat uk.bl.wa.hadoop.indexer.WARCIndexerReducer.reduce(WARCIndexerReducer.java:84)\n",
      "\tat uk.bl.wa.hadoop.indexer.WARCIndexerReducer.reduce(WARCIndexerReducer.java:29)\n",
      "\tat org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:469)\n",
      "\tat org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:417)\n",
      "\tat org.apache.hadoop.mapred.Child$4.run(Child.java:270)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:396)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1127)\n",
      "\tat org.apache.hadoop.mapred.Child.main(Child.java:264)\n",
      "Caused by: org.apache.solr.client.solrj.SolrServerException: No live SolrServers available to handle this request:[http://192.168.1.180:8988/solr/jisc3, http://192.168.1.180:8983/solr/jisc3, http://192.168.1.180:8996/solr/jisc3, http://192.168.1.180:8994/solr/jisc3]\n",
      "\tat org.apache.solr.client.solrj.impl.LBHttpSolrServer.request(LBHttpSolrServer.java:354)\n",
      "\tat org.apache.solr.client.solrj.impl.CloudSolrServer$1.call(CloudSolrServer.java:332)\n",
      "\tat org.apache.solr.client.solrj.impl.CloudSolrServer$1.call(CloudSolrServer.java:329)\n",
      "\tat java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:138)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)\n",
      "\tat java.lang.Thread.run(Thread.java:662)\n",
      "Caused by: org.apache.solr.client.solrj.impl.HttpSolrServer$RemoteSolrException: Cannot talk to ZooKeeper - Updates are disabled.\n",
      "\tat org.apache.solr.client.solrj.impl.HttpSolrServer.request(HttpSolrServer.java:495)\n",
      "\tat org.apache.solr.client.solrj.impl.HttpSolrServer.request(HttpSolrServer.java:199)\n",
      "\tat org.apache.solr.client.solrj.impl.LBHttpSolrServer.request(LBHttpSolrServer.java:285)\n",
      "\t... 7 more\n",
      "</pre>\n",
      "    \n",
      "\n",
      "So, it seems the issue is simply that of scale. The JISC collection has very large numbers of resources per input file, and this stretches the size of the mapper outputs to the system's limits.\n",
      "\n",
      "Trying one more run, switching on the PDF checker, and enabling compression, to see how it goes... That seemed to work! Output/temp files look compressed. Also, PDF Preflight tests appeared to add a negligible amount of processing (all done at 02:18:00). Shuffle & Merge seemed to work ok, although some disk space grumbling. Sadly, SOLRs still down, but 5min pause should help, I think. Restarting SOLRs... So, worked a bit, but killed the SOLRs pretty quickly.\n",
      "After 07:30:00 the merge was complete (although many finished before then - using the host as the key is not very balanced).\n",
      "BUT after 15:00:00 still locked with only some successful submission. A coupled of reducers got over 69%.\n",
      "\n",
      "Just realised all the reducers are running on the same nodes! That's not helping... Num reducers reduced to one?\n",
      "\n",
      "To confirm this, I'd like to understand what happened for the earlier indexing processes. For AADDA, I assume the difference is largely down to the links. For LDWA, it seems unclear, as I recall Roger processing that in one go!\n",
      "\n",
      "Ok, so confirmed with Roger that he did it in c.34 chunks of about about a thousand WARCs each (33,102 in total). Given there are 1.1 billion items in the index, this still seems to be rather good performance compared to what we see now. Perhaps the number of reducers per node was lower? Maybe only two, at 2GB each.\n",
      "\n",
      "Rough history.\n",
      "\n",
      "First, 1GB got both, and up to 8 of either mappers or reducers.\n",
      "Then, 2GB and 2/2 mappers/reducers.\n",
      "Currently, 1GB and 5/5 mappers/reducers.\n",
      "\n",
      "Timing is right for the LDWA index to be during the 2R/node period, which may well explain why that worked ok.\n",
      "\n",
      "Indexing on the Cluster\n",
      "=======================\n",
      "The second JISC2 index appears to have failed, so major change of tack required.\n",
      "\n",
      "With Lewis, I chopped up some Cloudera code and managed to build multiple shards directly on HDFS during the Reduce phase.\n",
      "\n",
      "More detail TBA.\n",
      "\n",
      "I then went to add this to webarchive-discovery, but it turned out that Solr was compiled against Hadoop 2.x.x and so to get the same logic to work I had to backport the HDFS part of solr-core::4.7.1 to Hadoop 0.20.2 (shading the modified classes over the top of the original one). Due to semantic subtleties about fsync(), hflush() and plain old sync(), and FileSystem.get() not being thread-safe unless a specific property is set to avoid caching it, the port took a while.\n",
      "\n",
      "However, now running on the shuf.1000 and it seems to work fine. Not actually terribly quick, though, as running all three shard reducers on one grunt! (grunt13). Need to re-config so there's e.g. only two reducers per grunt and bump up to the 26 shards (i.e. full production settings)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}