{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Link Half-Life\n",
      "==============\n",
      "\n",
      "The goal is to randomly sample URLs from previous years of the web archive, and for each one determine its current status. Has is long since gone? Or is it still online? With a few thousand URLs, it should be possible to build up a reasonably accurate picture of the 'half-life' of the URLs in our archive.\n",
      "\n",
      "This started as a Java tool in the Shine project, but it makes more sense to sketch it out here first. \n",
      "\n",
      "Initially, the possible statuses for the URIs were:\n",
      "\n",
      "~~~~ java\n",
      "\tpublic enum URIStatus {\n",
      "\t\t/* Successful request, returned the same content as the original. */\n",
      "\t\tSAME,\n",
      "\t\t/* Successful request, resulted in a redirect (307 etc.) to something which appears to be the same as the original. */\n",
      "\t\tMOVED_AND_SAME,\n",
      "\t\t/* Successful request, returns something which appears to be similar to the original. */\n",
      "\t\tSIMILAR,\n",
      "\t\t/* Successful request, resulted in a redirect (307 etc.) to something which appears to be similar to the original. */\n",
      "\t\tMOVED_AND_SIMILAR,\n",
      "\t\t/* Successful request, but resulted in a redirect (307 etc.) to something which appears to be different. */\n",
      "\t\tMOVED,\n",
      "\t\t/* Successful request, but no knowledge as to whether the content is the same. */\n",
      "\t\tFOUND,\n",
      "\t\t/* Successful request, but content gone (404 etc.) */\n",
      "\t\tNOT_FOUND,\n",
      "\t\t/* Host connected, but errored on request */\n",
      "\t\tSERVER_ERROR,\n",
      "\t\t/* Host resolves, but cannot connect */\n",
      "\t\tHOST_CONNECT_ERROR,\n",
      "\t\t/* Host resolves, but cannot be reached */\n",
      "\t\tHOST_UNREACHABLE,\n",
      "\t\t/* Host no longer resolves via DNS */\n",
      "\t\tHOST_UNRESOLVABLE,\n",
      "\t\t/* Host's domain is not registered. */\n",
      "\t\tHOST_UNREGISTERED\n",
      "\t}\n",
      "~~~~\n",
      "\n",
      "However, it is not clear that some of these classes are worth distinguishing at all times, so we proceed with a simpler taxonomy:\n",
      "\n",
      "* **OK** - Host and URL known: got a 200 response at the original URL\n",
      "* **MOVED** - Host and URL known: got a 200 response after following any 3xx redirects.\n",
      "* **MISSING** - Host is known, but URL returned a 404 or similar, after zero or more 3xx redirects.\n",
      "* **ERROR** - Host is known, but URL returned a 500 or similar, after zero or more 3xx redirects.\n",
      "* **GONE** - No connection possible (UNRESOLVABLE, UNREACHABLE, CONNECTION-REFUSED etc.).\n",
      "\n",
      "This works reasonably well, although it does not compare the contents, so the **OK** and **MOVED** really only refers to the URL itself. If we add the ability to determine similarity of the content, then we have:\n",
      "\n",
      "* **UNCHANGED** - Host and URL known: got an identical 200 response at the original URL\n",
      "* **MOVED** - Host and URL known: got an identical 200 response after following any 3xx redirects.\n",
      "* **CHANGED** - Host and URL known: got an different 200 response at the original URL\n",
      "* **REDIRECTED** - Host and URL known: got an different 200 response after following any 3xx redirects.\n",
      "* **MISSING** - Host is known, but URL returned a 404 or similar, after zero or more 3xx redirects.\n",
      "* **ERROR** - Host is known, but URL returned a 500 or similar, after zero or more 3xx redirects.\n",
      "* **GONE** - No connection possible (UNRESOLVABLE, UNREACHABLE, CONNECTION-REFUSED etc.).\n",
      "\n",
      "Although **CHANGED** and **REDIRECTED** could probably be merged really."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Checking The Status\n",
      "\n",
      "The `halflist.checkurl` python file implements the URL checker. It can return a range of cases as shown by the tests below. If only checks if the URL resolved to a 200, and does not attempt to verify the content has not been changed."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from halflife.checkurl import checkUrl\n",
      "\n",
      "# Some example URLs:\n",
      "urls = [('http://this-domain-does-not-exist.org/', 'UNRESOLVABLE' ),\n",
      "        ('http://explorer.bl.uk', 'TIMEOUT' ),\n",
      "        ('http://example.org/', 'OK' ),\n",
      "        ('http://example.org:79/', 'NOROUTE' ),\n",
      "        ('http://httpstat.us/302', 'REDIRECT+OK' ),\n",
      "        ('http://httpstat.us/404', 'GONE' ),\n",
      "        ('http://httpstat.us/500', 'ERROR') ]\n",
      "\n",
      "# Now check these example URLs:\n",
      "for url,state in urls:\n",
      "    newstate = checkUrl(url)\n",
      "    if state == newstate:\n",
      "        print( \"PASS \" + url + \" \" + state )\n",
      "    else:\n",
      "        print( \"FAIL \" + url + \" expected \" + state + \" got \" + newstate )\n",
      "        break\n",
      "        \n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "PASS http://this-domain-does-not-exist.org/ UNRESOLVABLE\n",
        "PASS http://explorer.bl.uk TIMEOUT"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "PASS http://example.org/ OK"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "PASS http://example.org:79/ NOROUTE"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "PASS http://httpstat.us/302 REDIRECT+OK"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "PASS http://httpstat.us/404 GONE"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "PASS http://httpstat.us/500 ERROR"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 53
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Sampling Solr\n",
      "-------------\n",
      "\n",
      "The first step is to randomly sample N links from each year from Solr. Faceting by year looks something like:\n",
      "\n",
      "<http://localhost:8080/discovery/select?q=*%3A*&wt=json&indent=true&rows=0&facet.range.gap=%2B1YEAR&facet.range=crawl_date&f.crawl_date.facet.range.start=1980-01-01T00:00:00Z&f.crawl_date.facet.range.end=2020-01-01T00:00:00Z&facet=on>\n",
      "\n",
      "Then something link this to sample randomly:\n",
      "\n",
      "<http://chrome.bl.uk:8080/solr/select/?q=*:*&rows=1&sort=random_2%20desc&fq=timestamp:[2004-01-01T00:00:00Z%20TO%202005-01-01T00:00:00Z]>\n",
      "\n",
      "In fact, the easiest way is simply to generate lots of random sample outputs and store the JSON offline, then process that.\n",
      "\n",
      "I wrote a script to do this, called `yearwise-sampler.py`.\n",
      "\n",
      "So, I randomly sampled 100 URLs from each year of the Solr index of the Selective Archive, and stored the files a sub-folder (`./halflife`).\n",
      "\n",
      "I can then run a suitable status checked against that sample.\n",
      "    "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from IPython.display import FileLink, FileLinks\n",
      "\n",
      "FileLinks('halflife/2014-08-01-Selective-Sample')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "halflife/2014-08-01-Selective-Sample/<br>\n",
        "&nbsp;&nbsp;<a href='files/halflife/2014-08-01-Selective-Sample/random-sample-for-2004.json' target='_blank'>random-sample-for-2004.json</a><br>\n",
        "&nbsp;&nbsp;<a href='files/halflife/2014-08-01-Selective-Sample/random-sample-for-2005.json' target='_blank'>random-sample-for-2005.json</a><br>\n",
        "&nbsp;&nbsp;<a href='files/halflife/2014-08-01-Selective-Sample/random-sample-for-2006.json' target='_blank'>random-sample-for-2006.json</a><br>\n",
        "&nbsp;&nbsp;<a href='files/halflife/2014-08-01-Selective-Sample/random-sample-for-2007.json' target='_blank'>random-sample-for-2007.json</a><br>\n",
        "&nbsp;&nbsp;<a href='files/halflife/2014-08-01-Selective-Sample/random-sample-for-2008.json' target='_blank'>random-sample-for-2008.json</a><br>\n",
        "&nbsp;&nbsp;<a href='files/halflife/2014-08-01-Selective-Sample/random-sample-for-2009.json' target='_blank'>random-sample-for-2009.json</a><br>\n",
        "&nbsp;&nbsp;<a href='files/halflife/2014-08-01-Selective-Sample/random-sample-for-2010.json' target='_blank'>random-sample-for-2010.json</a><br>\n",
        "&nbsp;&nbsp;<a href='files/halflife/2014-08-01-Selective-Sample/random-sample-for-2011.json' target='_blank'>random-sample-for-2011.json</a><br>\n",
        "&nbsp;&nbsp;<a href='files/halflife/2014-08-01-Selective-Sample/random-sample-for-2012.json' target='_blank'>random-sample-for-2012.json</a><br>\n",
        "&nbsp;&nbsp;<a href='files/halflife/2014-08-01-Selective-Sample/random-sample-for-2013.json' target='_blank'>random-sample-for-2013.json</a><br>\n",
        "&nbsp;&nbsp;<a href='files/halflife/2014-08-01-Selective-Sample/random-sample-for-2014.json' target='_blank'>random-sample-for-2014.json</a><br>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 55,
       "text": [
        "halflife/2014-08-01-Selective-Sample/\n",
        "  random-sample-for-2004.json\n",
        "  random-sample-for-2005.json\n",
        "  random-sample-for-2006.json\n",
        "  random-sample-for-2007.json\n",
        "  random-sample-for-2008.json\n",
        "  random-sample-for-2009.json\n",
        "  random-sample-for-2010.json\n",
        "  random-sample-for-2011.json\n",
        "  random-sample-for-2012.json\n",
        "  random-sample-for-2013.json\n",
        "  random-sample-for-2014.json"
       ]
      }
     ],
     "prompt_number": 55
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So, I have another script, [`sample-to-table.py`](./halflife/sample-to-table.py), that pulls this all together, and generated TSV data which is easy to slice and dice with command-line tools."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "FileLink('halflife/sample-v2-3.tsv')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<a href='files/halflife/sample-v2-3.tsv' target='_blank'>halflife/sample-v2-3.tsv</a><br>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 59,
       "text": [
        "/Users/andy/Documents/workspace/keeping-codes/experiments/halflife/sample-v2-3.tsv"
       ]
      }
     ],
     "prompt_number": 59
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So, a quick:\n",
      "\n",
      "    $ cut -f 1,4 sample-v2-3.tsv | sort | uniq -c\n",
      "\n",
      "And we have suitable data showing how the URLs have faired since being crawled. e.g.\n",
      "\n",
      "    $ cut -f 1,4 sample-v2-3.tsv | sort | uniq -c | grep OK\n",
      "     4 2004\tOK\n",
      "     8 2005\tOK\n",
      "     6 2006\tOK\n",
      "    11 2007\tOK\n",
      "    31 2008\tOK\n",
      "    47 2009\tOK\n",
      "    38 2010\tOK\n",
      "    50 2011\tOK\n",
      "    58 2012\tOK\n",
      "    53 2013\tOK"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So, using another Python script we can post-process into something graphable..."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import csv, sys, pprint\n",
      "import collections\n",
      "\n",
      "filename = 'halflife/sample-v2-3.tsv'\n",
      "counts = collections.defaultdict(lambda: collections.defaultdict(int))\n",
      "with open(filename, 'rb') as f:\n",
      "    reader = csv.reader(f, delimiter=\"\\t\")\n",
      "    for row in reader:\n",
      "        #print row\n",
      "        counts[row[0]][row[3]] += 1\n",
      "\n",
      "# Create the header line\n",
      "print(\"\\t\".join(['Year', 'GONE', 'ERROR', 'MISSING','MOVED','OK']))\n",
      "for year in sorted(counts.keys()):\n",
      "    # Now build up data\n",
      "    line = year\n",
      "    for state in ['GONE', 'ERROR', 'MISSING','MOVED','OK']:\n",
      "        line = line + \"\\t\" + str(counts[year][state])\n",
      "    print(line)\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Year\tGONE\tERROR\tMISSING\tMOVED\tOK\n",
        "2004\t44\t1\t50\t1\t4\n",
        "2005\t10\t1\t53\t28\t8\n",
        "2006\t22\t0\t48\t24\t6\n",
        "2007\t16\t1\t61\t11\t11\n",
        "2008\t15\t2\t35\t17\t31\n",
        "2009\t13\t2\t21\t17\t47\n",
        "2010\t19\t4\t21\t18\t38\n",
        "2011\t14\t5\t18\t13\t50\n",
        "2012\t9\t2\t18\t13\t58\n",
        "2013\t14\t4\t21\t8\t53\n"
       ]
      }
     ],
     "prompt_number": 87
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Which, as a graph, look like this:\n",
      "\n",
      "![Half-life plot as stacked area chart](files/halflife/halflife-oukwa-100-sml.png)\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Scaling up\n",
      "==========\n",
      "\n",
      "A thousand URLs per year should be enough to get c. 1% accuracy on these estimates, and the trends in the 100-per-year sample should be confirmed."
     ]
    }
   ],
   "metadata": {}
  }
 ]
}