---
title: The Zeroth Law of Digital Preservation
layout: default
categories: [fundamentals]
tags: [outline]
---

In the field of Thermodynamics, there was a law so obvious, so taken for granted, that no-one even realised it was a law at first. Only after discovering and first, second and third laws of Thermodynamics did X realise that each of those depended on another law, quite simply, that heat moves from X to Y. This was considered so fundamental that it was named the Zeroth Law.

I believe that the field of digital preservation has followed a similar path, and that the mathematical theory of communication outlined in X is so fundamental to every single digital preservation system and concept we have been using, that Shannon's noisy-channel coding theorem should be considered our Zeroth Law.


Set the scene, describe source coding theorem, and describe our 'channels'. 

...

Source coding theorem as backbone of what we already used, lead into what it tells us about preserving information if we consider a format migration 'channel'.

But this applies to everything - even the 'discrete' form applies as much to text and morse code as it does to WAV files and HTTP. So, [what's so special about digital preservation?][What is Special About Digital Preservation.html].

---

Applications, and R.I.

Shannon Entropy, Compression & Representation Information
---------------------------------------------------------

We see compression makes files smaller but increases their entropy.

But while low entropy is clearly low content, high entropy case is less clear.

High entropy tends towards random noise.

Thus, without flags, very compressed files become impossible to distinguish. The RI has been effectively moved outside of the object, 

Information volume fixed.

Errors and redundancy.

There is presumably a parallel to the previous case about adding redundant information and the amount you need on a given noisy channel.

Picture of an orange v. the phrase 'an orange'.

Interesting use evaluating complexity of Pictish symbols: http://rspa.royalsocietypublishing.org/content/early/2010/03/26/rspa.2010.0041.full

Should lead into Turing

### Entropy and Transformations ###
Look at uni-gram and di-gram entropy, as the latter should be more revealing due to the fact that order matters.

Is this a separate point, or is analysing transformations via entropy critical?

