---
title: What to Preserve?
layout: default
categories: [fundamentals]
tags: [stub]
publish: false
---

Underline assumptions, Keeping Codes, formats as malleable, etc.

The bitstream:  "...the bitstream is a powerful abstraction layer..."

http://blogs.loc.gov/digitalpreservation/2013/12/bitcurators-open-source-approach-an-interview-with-cal-lee/

Use, only then are we known:
http://theodi.org/blog/five-stages-of-data-grief

The Migration Line
------------------

Leading on from The Stack, and complementing the Performance and Information.

Migrations And Emulations
- Strategies are where you draw the migration bubble on the stack.
    - Standards Reliance is the implied default.
    - Also, migrate to supported environment is a double bubble.

Rothenberg Vernacular (Middle english example ) is about perception side.
o-ring data is about the extrapolation, not the data. very different major migration loss.
Rotherberg vernacular extraction example, cut-paste from emulator, is a migration.


 digital preservation for years. The most common 'preservation action' is porting. Calling it 'source code migration' underplays its importance.

http://drj11.wordpress.com/2013/09/01/on-compiling-34-year-old-c-code/


Backwards compatible, new versions run old stuff. Breaks in this, e.g. deprecation, are the issue.
REF that high-change-freq. chart from last years iPres.


ref The First Preservation Strategy: Format...

Spectra
-------

### Performance and Information

To outline the two halves of the communication, the performance and the understanding, or reference another page covering this.

That the difficulties in preserving access to digital media arise primarily because the access the item is mediated, not because they are digital. The same can be said of needing a speaker of an obscure language to understand a book. You can either keep the language alive (emulation), or translate the book (migration).

That RI is of two kinds:

1. Performance RI, that is the information required to reconstruct a particular interpretation of a digital artefact, i.e. to regenerate a signal to be interpreted by an actor outside the system. e.g. rendering a PDF.
2. Perception/Understanding/ RI, that is any futher information required to support the actor in understanding the preserved signal.

That actor-RI is subjective. It's consqeuences aare tjhat is should be layered rather that designed up-front, and turns into educational resources if you wait long enough

IMPORTANT: Identifiers and distinguishing 'this version created the item' from 'this is the intended interpretation'. e.g. DOC is tricky here, as writer version confused with reader version?

Then the choice is the spectrum of attempting to preserve the performance precisely, and therefore avoiding the interpretation side. OR attempting to preserve 'the information', and judging that we can judge whether a new performance achieves this.

What is a document etc:
http://people.ischool.berkeley.edu/~buckland/whatdoc.html
http://inkdroid.org/journal/2013/10/15/suzanne-briet-on-ada-lovelace-day/


http://www.theatlantic.com/health/archive/2013/02/our-comprehensive-living-archive-of-apples/273538/

Possible a War Story: http://gigaom.com/2014/01/14/the-search-for-the-lost-cray-supercomputer-os/

### Continuity and Archaeology

Another spectrum, from continuous, sustainable access to leave it and dig it up later.

Example, ideally, we just keep the stuff and allow researchers to dig it up.

Permanent Access
Digital Continuity
...

### Use and Usability

A fundamental aspect of known what we want to preserve with whether we are more concerned with the way a piece of software was used, or the ability to use the software.

- Choosing the performances: reference wikipedia black out case study
- The Performance Spectrum, just the one.
    - http://notepad.benfinoradin.info/2012/08/28/take-a-picture/

Contrast with physical? Is this a fundamental difference due to process nature of digital resources.

Diagram.

Examples.

Games, SecondLife, Facebook. Apps, dynamic websites etc.

Contrast with mature information carriers, PDF, JPEG, CSV, where we want to use the software that allows access to the data, but precise recovery of exact experience is ok?

Spectrum, web archiving somewhere in the middle. Partial functionality, partially user simulation.

A particular performance, the possible performances, the performances precisely.

Far end is full author environment stuff?

http://first-website.web.cern.ch/objectives/document-and-share-line-mode-browser-experience


Two dimensional?

Horizontal: Who to preserve?
A particular user, some particular group of users, some average user, most users, all users, (any user).
Vertical: Documentary - Simulation

When to Act?
------------
Something about monitoring user needs being more important in web archives, but monitoring technical environment (although that's basically users) also needed. As important as monitoring content?

How to Decide?
--------------

Lots of choices, but before leaping to conclusions, we need to consider who will decide.

1. easy issue reporting for those that want to. 
2. building access channels together with research groups.
3. anonymous monitoring of usage to determine preferences.


LINK TO [A perfect digital preservation experiment](A Perfect Digital Preservation Experiment.html)

Which should reveal the critical role of users etc.


LINK TO [Credible Threats](Credible Threats.html)

Threats, Threat models not defined, see Making Plans.

Trust models omitted - trust is social. see also http://en.wikipedia.org/wiki/Source_criticism

https://twitter.com/petemay/status/406397838529007616

We can imagine many risks, but following Rosenthal, we use the concept of 'threats' to underling the importance of knowing the likelyhood. Risks formally defined as likely hood combined with severity of outcome, but language helps avoid confusion over common usage.

* Format analysis - cf versions don't matter, features do.
* Hard disk quality risks - Losing half the array - CDROM of health data
* c.f. Advice for public sector TNA?

### Obsolescence

Given that is has proven possible to access the data, some might argue that the floppy disks and the data on them are not obsolete. I would say that this long, brittle chains of migration and extensive guesswork are exactly what obsolescence looks like. 

Even though a more efficient way to access the disks themselves is now available, the problem shifts understanding how to use the disks and the software, and how to access the data within.

It is the slow death of a thousand ambiguities, rather than an sudden, jarring expiration. Obsolescence is approached, rather than attained, with the costs of access rising every step of the way.

LINK TO [Making plans](Making Plans.html)